plot_input <- prop_true_small#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
}
plot_input <- prop_true_big#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
}
N_samples_small
prop_true_small
N_draws		<-	100000#
#
set.seed(407744)#
counts_mat <- apply(#
					X = prop_true_small, #
					MARGIN = 1, #
					FUN = function(x) #
						rmultinom(size = N_draws, prob = x, n = 1)#
				)#
#
# transpose#
counts <- t(counts_mat)
counts_mat
rowSums(counts_mat)
colSums(counts_mat)
counts_vec
# make into a vector#
counts_vec <- as.vector(counts)#
names(counts_vec) <- rep(x = taxon_names, each = samples_small)
# Original code by Ole Shelton#
# Hacked up by Jimmy O'Donnell#
# Comments mostly reflect Jimmy's imperfect understanding of the underlying concepts.#
#
library(gtools) # gtools::rdirichlet#
library(R2jags) # R2jags::jags#
# Suppose we have an aquarium which contains N_tax taxa#
N_tax <- 9#
#
# Give them names#
taxon_names <- paste("taxon_", LETTERS[1:N_tax], sep = "")#
# The distribution of DNA molecules derived from each of the taxa might look like the following#
# We call these alpha because they eventually become the alpha parameter of the Dirichlet distribution, which sorta determines how likely each component is to be drawn#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
set.seed(407744)#
DNA_alpha		<- sort(round(runif(N_tax,1,100)), decreasing = TRUE)#
names(DNA_alpha) <- taxon_names#
#
# those same values expressed as proportions#
DNA_alpha_prop <- DNA_alpha/sum(DNA_alpha)#
#############################################################################
# The actual simulation #
#############################################################################
#
# This simulates data that might be expected from high throughput sequencing of PCR amplicons generated from environmental samples: proportional abundance of sequences from each of the taxa/OTUs/dups#
# draw samples from the Dirichlet distribution, using alpha given by DNA_alpha to approximate the sampling of DNA molecules from the environment#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# This is incorrect: we call these proportions the "true" proportions because they represent the "true" proportional abundance of DNA from each taxon in the sample.#
# How many times should we sample?#
N_samples_small	<-	3#
set.seed(407744)#
prop_true_small <- rdirichlet(N_samples_small, DNA_alpha)#
colnames(prop_true_small) <- taxon_names#
#############
# OR...#
#############
N_samples_big		<- 100000#
set.seed(407744)#
prop_true_big <- rdirichlet(N_samples_big, DNA_alpha)#
colnames(prop_true_big) <- taxon_names#
# Set whichever of these as the plot input:#
plot_input <- prop_true_big#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
}#
# Using each row of the "true proportion" data frame as probabilities...#
# essentially: take a draw of 'N_draws' marbles, #
# and put them into each of some number of bins #
# with probability of going into each bin given by the "true proportion" row#
# (repeat n times)#
#
N_draws		<-	100000#
#
set.seed(407744)#
counts_mat <- apply(#
					X = prop_true_small, #
					MARGIN = 1, #
					FUN = function(x) #
						rmultinom(size = N_draws, prob = x, n = 1)#
				)#
#
# transpose#
counts <- t(counts_mat)#
#
# make into a vector#
counts_vec <- as.vector(counts)#
names(counts_vec) <- rep(x = taxon_names, each = N_samples_small)
counts_mat
#############################################################################
# This script simulates some data that resembles eDNA sequence data#
#############################################################################
#
# Original code by Ole Shelton#
# Hacked up by Jimmy O'Donnell#
# Comments mostly reflect Jimmy's imperfect understanding of the underlying concepts.#
#
library(gtools) # gtools::rdirichlet#
library(R2jags) # R2jags::jags#
# Suppose we have an aquarium which contains N_tax taxa#
N_tax <- 9#
# Give them names#
taxon_names <- paste("taxon_", LETTERS[1:N_tax], sep = "")#
# The distribution of DNA molecules derived from each of the taxa might look like the following#
# We call these alpha because they eventually become the alpha parameter of the Dirichlet distribution, which sorta determines how likely each component is to be drawn#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
set.seed(407744)#
DNA_alpha		<- sort(round(runif(N_tax,1,100)), decreasing = TRUE)#
names(DNA_alpha) <- taxon_names#
#
# those same values expressed as proportions#
DNA_alpha_prop <- DNA_alpha/sum(DNA_alpha)#
#############################################################################
# The actual simulation #
#############################################################################
#
# This simulates data that might be expected from high throughput sequencing of PCR amplicons generated from environmental samples: proportional abundance of sequences from each of the taxa/OTUs/dups#
# draw samples from the Dirichlet distribution, using alpha given by DNA_alpha to approximate the sampling of DNA molecules from the environment#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# This is incorrect: we call these proportions the "true" proportions because they represent the "true" proportional abundance of DNA from each taxon in the sample.#
# How many times should we sample?#
N_samples_small	<-	3#
set.seed(407744)#
prop_true_small <- rdirichlet(N_samples_small, DNA_alpha)#
colnames(prop_true_small) <- taxon_names#
#############
# OR...#
#############
N_samples_big		<- 100000#
set.seed(407744)#
prop_true_big <- rdirichlet(N_samples_big, DNA_alpha)#
colnames(prop_true_big) <- taxon_names#
# Set whichever of these as the plot input:#
plot_input <- prop_true_big#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
  abline(h = DNA_alpha_prop[i], col = "red")#
}
#############################################################################
# This script simulates some data that resembles eDNA sequence data#
#############################################################################
#
# Original code by Ole Shelton#
# Hacked up by Jimmy O'Donnell#
# Comments mostly reflect Jimmy's imperfect understanding of the underlying concepts.#
#
library(gtools) # gtools::rdirichlet#
library(R2jags) # R2jags::jags#
# Suppose we have an aquarium which contains N_tax taxa#
N_tax <- 9#
# Give them names#
taxon_names <- paste("taxon_", LETTERS[1:N_tax], sep = "")#
# The distribution of DNA molecules derived from each of the taxa might look like the following#
# We call these alpha because they eventually become the alpha parameter of the Dirichlet distribution, which sorta determines how likely each component is to be drawn#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
set.seed(407744)#
DNA_alpha		<- sort(round(runif(N_tax,1,100)), decreasing = TRUE)#
names(DNA_alpha) <- taxon_names#
#
# those same values expressed as proportions#
DNA_alpha_prop <- DNA_alpha/sum(DNA_alpha)#
#############################################################################
# The actual simulation #
#############################################################################
#
# This simulates data that might be expected from high throughput sequencing of PCR amplicons generated from environmental samples: proportional abundance of sequences from each of the taxa/OTUs/dups#
# draw samples from the Dirichlet distribution, using alpha given by DNA_alpha to approximate the sampling of DNA molecules from the environment#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# This is incorrect: we call these proportions the "true" proportions because they represent the "true" proportional abundance of DNA from each taxon in the sample.#
# How many times should we sample?#
N_samples_small	<-	3#
set.seed(407744)#
prop_true_small <- rdirichlet(N_samples_small, DNA_alpha)#
colnames(prop_true_small) <- taxon_names#
#############
# OR...#
#############
N_samples_big		<- 100000#
set.seed(407744)#
prop_true_big <- rdirichlet(N_samples_big, DNA_alpha)#
colnames(prop_true_big) <- taxon_names#
# Set whichever of these as the plot input:#
plot_input <- prop_true_big#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
  abline(v = DNA_alpha_prop[i], col = "red")#
}
N_tax
rexp(N_tax, rate = 1)
rexp(N_tax, rate = 2)
plot(sort(rexp(N_tax, rate = 2)))
par(mfrow = c(3,3))
for(i in 1:9){#
	plot(sort(rexp(20, rate = i)))#
}
for(i in 1:9){#
	plot(sort(rexp(n = 20, rate = i/10)))#
}
for(i in 1:9){#
	plot(sort(rexp(n = 20, rate = i)))#
}
for(i in 1:9){#
	plot(sort(rexp(n = 20, rate = i)), main = paste("rate = ", i, sep = ""))#
}
for(i in 1:9){#
	plot(sort(rexp(n = 100, rate = i)), main = paste("rate = ", i, sep = ""))#
}
for(i in 1:9){#
	plot(#
		sort(#
			rexp(#
				n = 100, #
				rate = i#
				)#
			)#
		, #
		main = paste("rate = ", i, sep = ""), #
		ylim = c(0,5)#
	)#
}
dpareto <- function(x, xm, alpha) ifelse(x > xm , alpha*xm**alpha/(x**(alpha+1)), 0)#
ppareto <- function(q, xm, alpha) ifelse(q > xm , 1 - (xm/q)**alpha, 0 )#
qpareto <- function(p, xm, alpha) ifelse(p < 0 | p > 1, NaN, xm*(1-p)**(-1/alpha))#
rpareto <- function(n, xm, alpha) qpareto(runif(n), xm, alpha)#
#
# The following function computes the MLE of the parameters#
pareto.mle <- function(x)#
{#
  xm <- min(x)#
  alpha <- length(x)/(sum(log(x))-length(x)*log(xm))#
  return( list(xm = xm, alpha = alpha))#
}#
# Compute Kolmogorov-Smirnov statistic, using parametric bootstrap to estimate the p-value.#
pareto.test <- function(x, B = 1e3)#
{#
  a <- pareto.mle(x)#
#
  # KS statistic#
  D <- ks.test(x, function(q) ppareto(q, a$xm, a$alpha))$statistic#
#
  # estimating p value with parametric bootstrap#
  B <- 1e5#
  n <- length(x)#
  emp.D <- numeric(B)#
  for(b in 1:B)#
  {#
    xx <- rpareto(n, a$xm, a$alpha);#
    aa <- pareto.mle(xx)#
    emp.D[b] <- ks.test(xx, function(q) ppareto(q, aa$xm, aa$alpha))$statistic#
  }#
#
  return(list(xm = a$xm, alpha = a$alpha, D = D, p = sum(emp.D > D)/B))#
}#
# Example usage:#
# values from a Pareto distribution...#
x <- rpareto(100, 0.5, 2)#
pareto.test(x)
pareto.test(x)
x <- rchisq(100, df=2)
pareto.test(x)
my_number <- 4#
par_scale <- 1 # scale parameter "my_number sub m" aka "xm" -- the minimum possible value of random_variable#
par_shape <- 3 # shape parameter "alpha"
Prob_rand_gt_my <- if(my_number >= par_scale){#
						(par_scale/my_number)^par_shape#
					} else { 1 }
Prob_rand_gt_my
for(i in 1:9){#
	plot(#
		sort(#
			rexp(#
				n = 100, #
				rate = i#
				)#
			)#
		, #
		main = paste("rate = ", i, sep = ""), #
		ylim = c(0,5), #
		pch = 20#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rexp(#
				n = 100, #
				rate = i#
				)#
			)#
		, #
		main = paste("rate = ", i, sep = ""), #
		ylim = c(0,5), #
		pch = 20, #
		cex = 0.5#
	)#
}
rexp(5, 1)
rexp(5, 2)
rexp(5, 3)
rexp(5, 4)
rexp(5, 10)
rexp(5, 0.1)
seq(0.1, 0.9, by = 0.1)
for(i in seq(0.1, 0.9, by = 0.1)){#
	plot(#
		sort(#
			rexp(#
				n = 100, #
				rate = i#
				)#
			)#
		, #
		main = paste("rate = ", i, sep = ""), #
		ylim = c(0,5), #
		pch = 20, #
		cex = 0.5#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rexp(#
				n = 100, #
				rate = i#
				)#
			)#
		, #
		main = paste("rate = ", i, sep = ""), #
		ylim = c(0,5), #
		pch = 20, #
		cex = 0.5#
	)#
}
par(mfrow = c(3,3))
rpareto(9, 1, 1)
par(mfrow = c(3,3))
plot(sort(rpareto(9, 1, 1)))
plot(sort(rpareto(20, 1, 1)))
replicate(10, sapply(rpareto(100, 1, 1), FUN = max))
rpareto(100, 1, 1)
replicate(10, max(rpareto(100, 1, 1))
)
replicate(10, max(rpareto(100, 1, 1)))
plot(replicate(10, max(rpareto(100, 1, 1))))
plot(max(replicate(10, rpareto(100, 1, 1))))
max(replicate(10, rpareto(100, 1, 1)))
plot(sapply(replicate(10, rpareto(100, 1, 1)), max))
rpareto(100, 1, 1)
replicate(10, rpareto(100, 1, 1)
)
plot(sapply(replicate(10, rpareto(100, 1, 1)), max))
plot(sort(rpareto(20, 1, 1)))
plot(sort(rpareto(100, 1, 1)))
plot(sort(rpareto(100, 2, 1)))
plot(sort(rpareto(100, 3, 1)))
plot(sort(rpareto(100, 4, 1)))
plot(sort(rpareto(100, 5, 1)))
plot(sort(rpareto(100, 1, 1)))
plot(sort(rpareto(100, 1, 2)))
plot(sort(rpareto(100, 1, 3)))
plot(sort(rpareto(100, 1, 4)))
plot(sort(rpareto(100, 1, 5)))
plot(sort(rpareto(100, 1, 6)))
plot(sort(rpareto(100, 1, 2)))
plot(sort(rpareto(100, 1, 3)))
plot(sort(rpareto(100, 1, 4)))
plot(sort(rpareto(100, 1, 4)), ylim = c(0, 15))
plot(sort(rpareto(100, 1, 3)), ylim = c(0, 15))
plot(sort(rpareto(100, 1, 2)), ylim = c(0, 15))
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				100, i, 2#
			)#
		)#
		, ylim = c(0, 15)#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				100, 1, i#
			)#
		)#
		, ylim = c(0, 15)#
	)#
}#
# as alpha increases, maximum value drawn tends to decrease
runif(N_tax,1,100)
round(runif(N_tax,1,100))
counts_vec
counts_mat
barplot(counts_mat)
barplot(prop_true_small)
prop_true_small
barplot(t(prop_true_small))
barplot(counts_mat)
barplot(t(prop_true_small))
colsums(counts_mat)
colSums(counts_mat)
prop_true_small*1e5
counts_mat
t(counts_mat)
t(counts_mat)/rowSums(counts_mat)
ks.test(t(counts_mat)/rowSums(counts_mat), prop_true_small)
ks.test((counts_mat)/rowSums(counts_mat), prop_true_small)
counts_mat
t(counts_mat)/rowSums(t(counts_mat))
rowSums(t(counts_mat)/rowSums(t(counts_mat)))
rowSums(prop_true_small)
ks.test(t(counts_mat)/rowSums(t(counts_mat)), prop_true_small)
prop_true_small
counts_vec
counts_mat
counts
names(counts_mat) <- taxon_names
counts_mat
set.seed(407744)#
counts_mat <- apply(#
					X = eDNA_counts_small, #
					MARGIN = 1, #
					FUN = function(x) #
						rmultinom(size = N_draws, prob = x, n = 1)#
				)
#############################################################################
# This script simulates some data that resembles eDNA sequence data#
#############################################################################
#
# Original code by Ole Shelton#
# Hacked up by Jimmy O'Donnell#
# Comments mostly reflect Jimmy's imperfect understanding of the underlying concepts.#
#
library(gtools) # gtools::rdirichlet#
library(R2jags) # R2jags::jags#
# Suppose we have an aquarium which contains N_tax taxa#
N_tax <- 9#
# Give them names#
taxon_names <- paste("taxon_", LETTERS[1:N_tax], sep = "")#
# The distribution of DNA molecules derived from each of the taxa might look like the following#
# Ole originally called these alpha because they eventually become the alpha parameter of the Dirichlet distribution, which sorta determines how likely each component is to be drawn#
# I call these "true" because they represent the "true" abundance of DNA from each taxon in the sample.#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# Instead of runif, you might try using rexp or random from the Pareto distribution to more accurately resemble "real" eDNA results#
set.seed(407744)#
DNA_truth		<- sort(round(runif(N_tax,1,100)), decreasing = TRUE)#
names(DNA_truth) <- taxon_names#
#
# those same values expressed as proportions#
DNA_truth_prop <- DNA_truth/sum(DNA_truth)#
#############################################################################
# The actual simulation #
#############################################################################
#
# This simulates data that might be expected from high throughput sequencing of PCR amplicons generated from environmental samples: proportional abundance of sequences from each of the taxa/OTUs/dups#
# draw samples from the Dirichlet distribution, using alpha given by DNA_truth to approximate the sampling of DNA molecules from the environment#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# How many times should we sample?#
N_samples_small	<-	3#
set.seed(407744)#
eDNA_counts_small <- rdirichlet(N_samples_small, DNA_truth)#
colnames(eDNA_counts_small) <- taxon_names#
#############
# OR...#
#############
N_samples_big		<- 100000#
set.seed(407744)#
eDNA_counts_big <- rdirichlet(N_samples_big, DNA_truth)#
colnames(eDNA_counts_big) <- taxon_names#
# Set whichever of these as the plot input:#
plot_input <- eDNA_counts_big#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
  abline(v = DNA_truth_prop[i], col = "red")#
}#
# In a similar way, we can use each of the samples to inform the probabilities of draws from a multinomial distribution. This produces data that is functionally indistinguishable from that presented above.#
# Using each row of the "true proportion" data frame as probabilities...#
# essentially: take a draw of 'N_draws' marbles, #
# and put them into each of some number of bins #
# with probability of going into each bin given by the "true proportion" row#
# (repeat n times)#
#
N_draws		<-	100000#
#
set.seed(407744)#
counts_mat <- apply(#
					X = eDNA_counts_small, #
					MARGIN = 1, #
					FUN = function(x) #
						rmultinom(size = N_draws, prob = x, n = 1)#
				)
counts_mat
rownames(counts_mat) <- taxon_names
counts_mat
counts <- t(counts_mat)
counts
counts_vec <- as.vector(counts)
counts_vec
names(counts_vec) <- rep(x = taxon_names, each = N_samples_small)
counts_vec
mydata <- counts
mydata
dput(mydata)
counts <- mydata
colnames(counts)
names(counts_vec) <- rep(x = colnames(counts), each = nrow(counts))
counts_vec
nrow(counts)
length(counts)
length(counts_vec)
library(gtools) # gtools::rdirichlet#
library(R2jags) # R2jags::jags#
# Suppose we have an aquarium which contains N_tax taxa#
N_tax <- 9#
# Give them names#
taxon_names <- paste("taxon_", LETTERS[1:N_tax], sep = "")#
# The distribution of DNA molecules derived from each of the taxa might look like the following#
# Ole originally called these alpha because they eventually become the alpha parameter of the Dirichlet distribution, which sorta determines how likely each component is to be drawn#
# I call these "true" because they represent the "true" abundance of DNA from each taxon in the sample.#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# Instead of runif, you might try using rexp or random from the Pareto distribution to more accurately resemble "real" eDNA results#
set.seed(407744)#
DNA_truth		<- sort(round(runif(N_tax,1,100)), decreasing = TRUE)#
names(DNA_truth) <- taxon_names#
#
# those same values expressed as proportions#
DNA_truth_prop <- DNA_truth/sum(DNA_truth)#
#############################################################################
# The actual simulation #
#############################################################################
#
# This simulates data that might be expected from high throughput sequencing of PCR amplicons generated from environmental samples: proportional abundance of sequences from each of the taxa/OTUs/dups#
# draw samples from the Dirichlet distribution, using alpha given by DNA_truth to approximate the sampling of DNA molecules from the environment#
# Set seed to be able to reproduce draws from a probability distribution (gets reset each time one of these functions is used)#
# How many times should we sample?#
N_samples_small	<-	3#
set.seed(407744)#
eDNA_counts_small <- rdirichlet(N_samples_small, DNA_truth)#
colnames(eDNA_counts_small) <- taxon_names#
#############
# OR...#
#############
N_samples_big		<- 100000#
set.seed(407744)#
eDNA_counts_big <- rdirichlet(N_samples_big, DNA_truth)#
colnames(eDNA_counts_big) <- taxon_names#
# Set whichever of these as the plot input:#
plot_input <- eDNA_counts_big#
#
# Create plot layout (this looks complicated to allow it to be flexible to different taxa numbers)#
plot_cols <- ceiling(sqrt(ncol(plot_input)))#
plot_rows <- ceiling(ncol(plot_input)/plot_cols)#
plot_layout <- matrix(data = 1:(plot_cols*plot_rows), nrow = plot_rows, byrow = TRUE)#
layout(plot_layout)#
#
# plot histograms of the frequency of proportions sampled for each taxon.#
for (i in 1:ncol(plot_input)){#
  hist(plot_input[, i], main = taxon_names[i], xlab = "proportion")#
  abline(v = DNA_truth_prop[i], col = "red")#
}#
# In a similar way, we can use each of the samples to inform the probabilities of draws from a multinomial distribution. This produces data that is functionally indistinguishable from that presented above.#
# Using each row of the "true proportion" data frame as probabilities...#
# essentially: take a draw of 'N_draws' marbles, #
# and put them into each of some number of bins #
# with probability of going into each bin given by the "true proportion" row#
# (repeat n times)#
#
N_draws		<-	100000#
#
set.seed(407744)#
counts_mat <- apply(#
					X = eDNA_counts_small, #
					MARGIN = 1, #
					FUN = function(x) #
						rmultinom(size = N_draws, prob = x, n = 1)#
				)#
#
# assign names#
rownames(counts_mat) <- taxon_names#
#
# transpose#
counts <- t(counts_mat)#
#
mydata <- counts
counts <- mydata#
#
# make into a vector#
counts_vec <- as.vector(counts)#
names(counts_vec) <- rep(x = colnames(counts), each = nrow(counts))#
# want to save the file?
write.csv(x = counts, file = "counts.csv")
write.csv(x = counts, file = "counts.csv", row.names = FALSE)
write.csv(x = counts, file = "counts.csv", row.names = FALSE, quotes = FALSE)
write.csv(x = counts, file = "counts.csv", row.names = FALSE, quote = FALSE)
counts <- mydata
counts_vec <- as.vector(counts)#
names(counts_vec) <- rep(x = colnames(counts), each = nrow(counts))
as.matrix(mydata)
identical(counts, as.matrix(mydata))
mydata <- as.matrix(read.csv("counts.csv"))
identical(mydata, counts)
par(mfrow = c(3,3))#
for(i in 1:9){#
	plot(#
		sort(#
			rexp(#
				n = 100, #
				rate = i#
				)#
			)#
		, #
		main = paste("rate = ", i, sep = ""), #
		ylim = c(0,5), #
		pch = 20, #
		cex = 0.5#
	)#
}
dpareto <- function(x, xm, alpha) ifelse(x > xm , alpha*xm**alpha/(x**(alpha+1)), 0)#
ppareto <- function(q, xm, alpha) ifelse(q > xm , 1 - (xm/q)**alpha, 0 )#
qpareto <- function(p, xm, alpha) ifelse(p < 0 | p > 1, NaN, xm*(1-p)**(-1/alpha))#
rpareto <- function(n, xm, alpha) qpareto(runif(n), xm, alpha)#
#
par(mfrow = c(3,3))#
#
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				100, i, 2#
			)#
		)#
		, ylim = c(0, 15)#
	)#
}#
# xm simply determines the minimum value drawn#
#
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				100, 1, i#
			)#
		)#
		, ylim = c(0, 15)#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				100, i, 2#
			)#
		)#
		, ylim = c(0, 15)#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				100, i, 2#
			)#
		)#
		, ylim = c(0, 15), #
		main = paste("xm = ", i, sep = "")#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				n = 100, xm = 1, alpha = i#
			)#
		)#
		, ylim = c(0, 15), #
		main = paste("alph = ", i, sep = "")#
	)#
}
for(i in 1:9){#
	plot(#
		sort(#
			rpareto(#
				n = 100, xm = 1, alpha = i#
			)#
		)#
		, ylim = c(0, 15), #
		main = paste("alpha = ", i, sep = "")#
	)#
}
#(April 23 2014).#
netcascade <- function(imatrix,ranim,rplants,deadPlants=NULL, deadAnimals=NULL, targetGuild,target,return.matrix=F){#
  #---------ARGUMENT CHECKS-----------------------------------#
  if(class(imatrix)!="matrix" || (nrow(imatrix)+ncol(imatrix))<3){stop("'imatrix' must be an object of class 'matrix', with animal species on rows, plant species on columns and at least three species overall")}#
  if(class(ranim)!="numeric" || class(rplants) != "numeric" || max(c(max(ranim),max(rplants)))>1 || min(c(min(ranim),min(rplants)))<0){stop("'ranim' & 'rplants' must be numeric vectors with values ranging between 0 and 1")}#
  if((targetGuild%in%c("animal","plant"))==F){stop('Invalid target guild for primary extinction. Valid targets guilds are "animal" and "plant"')}#
  if(is.numeric(target)==F){stop('Invalid value for the "target" argument. You may specify a single species by entering its row or column number or you may use a vector of relative probabilites for all species in the target guild.')}#
  if(is.null(deadAnimals)==F && class(deadAnimals)!= "integer"){stop("deadAnimals must be either NULL or an integer vector specifying the row numbers of animals considered to be extinct on the original matrix")}#
  if(is.null(deadPlants)==F && class(deadPlants)!= "integer"){stop("deadPlants must be either NULL an integer vector specifying the column numbers of plants considered to be extinct on the original matrix")}#
  if(length(ranim)!= nrow(imatrix)){stop("The length of vector'ranim' must be equal to number of rows (i.e. animal species) in 'imatrix'")}#
  if(length(rplants)!= ncol(imatrix)){stop("The length of vector'rplants' must be equal to number of columns (i.e. plant species) in 'imatrix'")}#
  #---------DEFINING SOME VARIABLES---------------------------#
  nanim <- nrow(imatrix)#
  npla <- ncol(imatrix)#
  plants <- 1:npla;#
  animals <- 1:nanim#
  plantNA <- 1:npla#
  animNA <- 1:nanim#
  plantNA[deadPlants] <- NA#
  animNA[deadAnimals] <- NA #
  degree_when_lost_plants <- c()#
  degree_when_lost_animals <- c()#
  #----------CALCULATING DEPENDENCE MATRICES-------------------#
  M <- array(0,dim=c(nanim,npla,2))#
  for(i in 1:npla){#
    M[,i,1] <- imatrix[,i]/sum(imatrix[,i])#
  } #matrix of plant dependence on each animal#
  for(i in 1:nanim){#
    M[i,,2] <- imatrix[i,]/sum(imatrix[i,])#
  } #matrix of animal dependence on each plant#
  #-----------CHOOSING TARGET SPECIES FOR PRIMARY EXTINCTION---#
  coext_animals <- c()#
  coext_plants <- c()#
  if(length(target)==1){#
    if(targetGuild=="animal"){#
      if(target %in% deadAnimals){stop('Specified target species for the primary extinction is already extinct')}#
      coext_animals <- target#
      degree_when_lost_animals <- 1 #stores the degree of the extinction event of every animal species lost during the coextinction cascade. #
    }#
    if(targetGuild=="plant"){#
      if(target %in% deadPlants){stop('Specified target species for the primary extinction is already extinct')}#
      coext_plants <- target#
      degree_when_lost_plants <- 1#
    }#
  }else{#
    nspecies <- switch(targetGuild,animal = nanim, plant = npla)#
    if(length(target)==nspecies){#
      if(targetGuild =="animal"){#
        alive <- animals[is.na(animNA)==F]#
        coext_animals <- sample(c(alive,0),1,prob = c(target[is.na(animNA)==F],0))#
        degree_when_lost_animals <- 1#
      }#
      if(targetGuild =="plant"){#
        alive <- plants[is.na(plantNA)==F]#
        coext_plants <- sample(c(alive,0),1,prob = c(target[is.na(plantNA)==F],0))#
        degree_when_lost_plants <- 1#
      }#
    }else{#
      stop('Length of "target" must be 1 (specifying a single species within the target guild) or else be equal to the number of species in the target guild (specifying probabilities of primary extinction for each species in the target guild)')#
    }#
  }#
  imatrix[coext_animals,] <- 0 #
  imatrix[,coext_plants] <- 0#
  lostanimals <- coext_animals #final list of animals which were "alive" in the original community but became extinct during this primary extinction + extinction cascade#
  lostplants <- coext_plants #
  #-------------------CASCADE LOOP---------------------------#
  equilibrium <- FALSE#
  degree <- 1#
  degree_table <- data.frame(degree,guild=factor(targetGuild,levels=c("animal","plant")),n_extinctions=1)#
  while(equilibrium == FALSE){#
    ext_animals <- coext_animals#
    ext_plants <- coext_plants#
    plantNA[ext_plants] <- NA#
    animNA[ext_animals] <- NA#
    aleft <- animals[is.na(animNA)==F]#
    pleft <- plants[is.na(plantNA)==F]#
    if(length(ext_animals)>0){#
      for(i in 1:length(ext_animals)){#
        unlucky <- rplants[pleft]*M[ext_animals[i],pleft,1] > runif(length(pleft))#
        coext_plants = c(coext_plants,pleft[unlucky])#
      }#
      coext_animals <- c()#
      coext_plants <- unique(coext_plants)#
      plantNA[coext_plants] <- NA#
      lostplants <- c(lostplants,coext_plants)#
      imatrix[,coext_plants] <- 0#
      for(i in 1:npla){#
        if(sum(imatrix[,i])==0){#
          M[,i,1] <- 0#
        }else{#
          M[,i,1] <- imatrix[,i]/sum(imatrix[,i])#
        }#
      }#
      if(length(coext_plants)>0){#
        degree <- degree + 1#
        degree_when_lost_plants <- c(degree_when_lost_plants, rep(degree,length(coext_plants)))#
        degree_table[degree,] <- data.frame(degree,"plant",length(coext_plants))#
      }#
    }else{#
      for(i in 1:length(ext_plants)){#
        unlucky <- ranim[aleft]*M[aleft,ext_plants[i],2] > runif(length(aleft))#
        coext_animals <- c(coext_animals, aleft[unlucky])#
      }#
      coext_plants = c();#
      coext_animals <- unique(coext_animals)#
      lostanimals <- c(lostanimals,coext_animals)#
      animNA[coext_animals] <- NA#
      imatrix[coext_animals,] <- 0#
      for(i in 1:nanim){#
        if(sum(imatrix[i,])==0){#
          M[i,,2] <- 0#
        }else{#
          M[i,,2] <- imatrix[i,]/sum(imatrix[i,])#
        }#
      }#
      if(length(coext_animals)>0){#
        degree <- degree + 1#
        degree_when_lost_animals <- c(degree_when_lost_animals, rep(degree,length(coext_animals)))#
        degree_table[degree,] <- data.frame(degree,"animal",length(coext_animals))  #
      }#
    }#
    equilibrium <- equilibrium + (length(coext_plants)+length(coext_animals))==0#
  }#
  #-------------------OUTPUT---------------------------#
  if(return.matrix==T){#
    return(list(interaction_matrix = imatrix, lost_animals = lostanimals, lost_plants = lostplants))  #
  }else{#
    if(length(lostanimals)>0){#
      spp_data_animals <- data.frame(lost_animal = lostanimals,degree_of_extinction=degree_when_lost_animals)#
    }else{#
      spp_data_animals <- "No animal species were lost"#
    }#
    if(length(lostplants)>0){#
      spp_data_plants <- data.frame(lost_plant = lostplants,degree_of_extinction=degree_when_lost_plants)#
    }else{#
      spp_data_plants <- "No plant species were lost"#
    }#
    return(list(cascade_data=degree_table,animal_species_data=spp_data_animals,plant_species_data=spp_data_plants))#
  }#
}
install.packages("vegan")
library(vegan)
install.packages("bipartite")
library(bipartite)
??data::vegan
??bipartite
m=10#
n=5#
mat=matrix(rbinom(100,10,0.1),m,n)#
#
coextDeg(mat,0.9,1,100) #extinction degree (the number of times one extinction spreads to the other set of species)#
coextNumber(mat,0.1,0.4,100)#number of coextinction
#coextNumber performs simulations (n = nsims) of a single episode of primary extinction and its possible associated coextinction cascade and creates a frequency distribution for the total number of extinctions for each episode.#
#primary extinctions are uniform random among both groups.'r' is sampled from a interval (rlow, rup)#
coextNumber <- function(imatrix,rlow,rup,nsims){#
  ext_counts <- c()#
  for(sim in 1:nsims){#
    rvalue <- runif(1,rlow,rup)#
    ranim <- rep(rvalue, nrow(imatrix))#
    rplants <- rep(rvalue, ncol(imatrix))#
    guild <- sample(c('animal','plant'),1,F,c(nrow(imatrix),ncol(imatrix)))#
    if(guild=="animal"){#
      target <- rep(1,nrow(imatrix))#
    }else{#
      target <- rep(1,ncol(imatrix))#
    }    #
    sim_results <- netcascade(imatrix,ranim = ranim, rplants = rplants, targetGuild = guild, target = target)#
    ext_counts[sim] <- sum(sim_results[[1]]$n_extinctions)#
  }#
  return(ext_counts)#
}
#coextMaxLvl performs simulations (n = nsims) of a single episode of primary extinction and its possible associated coextinction cascade and returns how often the extinction sequence ended in each extinction level (i.e. first level, second level, third level, etc.) #
#primary extinctions are uniform random among both groups.'r' is sampled from a interval (rlow, rup)#
coextDeg <- function(imatrix,rlow,rup,nsims){#
  degs <- c()#
  for(sim in 1:nsims){#
    rvalue <- runif(1,rlow,rup)#
    ranim <- rep(rvalue, nrow(imatrix))#
    rplants <- rep(rvalue, ncol(imatrix))#
    guild <- sample(c('animal','plant'),1,F,c(nrow(imatrix),ncol(imatrix)))#
    if(guild=="animal"){#
      target <- rep(1,nrow(imatrix))#
    }else{#
      target <- rep(1,ncol(imatrix))#
    }   #
    profiles <- netcascade(imatrix=imatrix,ranim=ranim,rplants=rplants,targetGuild=guild,target=target)#
    degs[sim] <- max(profiles[[1]]$degree)#
  }#
  return(degs)#
}
m=10#
n=5#
mat=matrix(rbinom(100,10,0.1),m,n)
mat
coextDeg(mat,0.9,1,100) #extinction degree (the number of times one extinction spreads to the other set of species)#
coextNumber(mat,0.1,0.4,100)#number of coextinction
source("coextDeg_mod.R", "coextNumber_mod.R")
source("coextDeg_mod.R")
source("coextDeg_mod.R")
setwd("/Users/threeprime/Documents/Dropbox/NIMBioS END/extinctions/Analysis")
source("coextDeg_mod.R")
coextDeg_mod <- source("coextDeg_mod.R")
coextDeg_mod
stop("hi!")
stuff <- "poop"
stop("hi!", stuff)
dist_call <- rnorm
dist_call(3)
if(dist == "normal"){#
    		dist_call <- rnorm#
    	} else if(dist == "exponential"){#
    		dist_call <- rexp#
    	} else if(dist == "uniform"){#
    		dist_call <- runif#
    	}
dist
distr
distr <- "normal"
if(distr == "normal"){#
    		dist_call <- rnorm#
    	} else if(distr == "exponential"){#
    		dist_call <- rexp#
    	} else if(distr == "uniform"){#
    		dist_call <- runif#
    	}
dist_call
distr <- "uniform"
if(distr == "normal"){#
    		dist_call <- rnorm#
    	} else if(distr == "exponential"){#
    		dist_call <- rexp#
    	} else if(distr == "uniform"){#
    		dist_call <- runif#
    	}
dist_call
distr <- "normal"
if(distr == "normal"){#
    		dist_call <- rnorm#
    	} else if(distr == "exponential"){#
    		dist_call <- rexp#
    	} else if(distr == "uniform"){#
    		dist_call <- runif#
    	}
dist_call
rexp(3)
rexp(3)
stuff <- runif(3)
stuff
rbeta(3)
rbeta(3, shape1 = 2, shape2 = 3)
plot(rbeta(100, shape1 = 2, shape2 = 2))
hist(rbeta(100, shape1 = 2, shape2 = 2))
hist(rbeta(100, shape1 = 2, shape2 = 2))
hist(rbeta(100, shape1 = 2, shape2 = 2))
hist(rbeta(100, shape1 = 1, shape2 = 1))
hist(rbeta(1000, shape1 = 1, shape2 = 1))
hist(rbeta(10000, shape1 = 1, shape2 = 1))
hist(rbeta(10000, shape1 = 2, shape2 = 1))
hist(rbeta(10000, shape1 = 2, shape2 = 0.5))
hist(rbeta(10000, shape1 = 1, shape2 = 0.5))
hist(rbeta(10000, shape1 = 1, shape2 = -0.5))
hist(rbeta(10000, shape1 = 1, shape2 = 0.75))
hist(rbeta(10000, shape1 = 1, shape2 = 0.5))
hist(rbeta(10000, shape1 = 1, shape2 = 0.25))
?rbeta
coextDeg <- function(imatrix, nsims, beta_par1 = , beta_par2 = ){#
  degs <- c()#
  for(sim in 1:nsims){#
    ranim <- rbeta(n = nrow(imatrix), shape1 = beta_par1, shape2 = beta_par2)#
    rplants <- rbeta(n = ncol(imatrix), shape1 = beta_par1, shape2 = beta_par2)#
    guild <- sample(c('animal','plant'),1,F,c(nrow(imatrix),ncol(imatrix)))#
    if(guild=="animal"){#
      target <- rep(1,nrow(imatrix))#
    }else{#
      target <- rep(1,ncol(imatrix))#
    }   #
    profiles <- netcascade(imatrix=imatrix,ranim=ranim,rplants=rplants,targetGuild=guild,target=target)#
    degs[sim] <- max(profiles[[1]]$degree)#
  }#
  return(degs)#
}
coextDeg <- function(imatrix, nsims, beta_par1 = , beta_par2 = ){
coextDeg <- function(imatrix, nsims, beta_par1 = 1, beta_par2 = 1){#
  degs <- c()#
  for(sim in 1:nsims){#
    ranim <- rbeta(n = nrow(imatrix), shape1 = beta_par1, shape2 = beta_par2)#
    rplants <- rbeta(n = ncol(imatrix), shape1 = beta_par1, shape2 = beta_par2)#
    guild <- sample(c('animal','plant'),1,F,c(nrow(imatrix),ncol(imatrix)))#
    if(guild=="animal"){#
      target <- rep(1,nrow(imatrix))#
    }else{#
      target <- rep(1,ncol(imatrix))#
    }   #
    profiles <- netcascade(imatrix=imatrix,ranim=ranim,rplants=rplants,targetGuild=guild,target=target)#
    degs[sim] <- max(profiles[[1]]$degree)#
  }#
  return(degs)#
}
coextDeg(mat, 100)
coextDeg(mat, 100, 2, 1)
#coextNumber performs simulations (n = nsims) of a single episode of primary extinction and its possible associated coextinction cascade and creates a frequency distribution for the total number of extinctions for each episode.#
#primary extinctions are uniform random among both groups.'r' is sampled from a interval (rlow, rup)#
#
coextNumber <- function(imatrix, nsims, beta_par1 = 1, beta_par2 = 1){#
  ext_counts <- c()#
  for(sim in 1:nsims){#
    ranim <- rbeta(n = nrow(imatrix), shape1 = beta_par1, shape2 = beta_par2)#
    rplants <- rbeta(n = ncol(imatrix), shape1 = beta_par1, shape2 = beta_par2)#
    guild <- sample(c('animal','plant'),1,F,c(nrow(imatrix),ncol(imatrix)))#
    if(guild=="animal"){#
      target <- rep(1,nrow(imatrix))#
    }else{#
      target <- rep(1,ncol(imatrix))#
    }    #
    sim_results <- netcascade(imatrix,ranim = ranim, rplants = rplants, targetGuild = guild, target = target)#
    ext_counts[sim] <- sum(sim_results[[1]]$n_extinctions)#
  }#
  return(ext_counts)#
}
coextDeg <- source("coextDeg_mod.R")
coextDeg <- source("coextDeg_mod.R")#
coextNumber <- source("coextNumber_mod.R")
m <- 10#
#
# let n be the number of species in Trophic level 2#
n <- 5
mat=matrix(rbinom(100,10,0.1),m,n)
coextDeg(mat,0.9,1,100) #extinction degree (the number of times one extinction spreads to the other
coextDeg
coextDeg
source("coextDeg_mod.R")#
source("coextNumber_mod.R")
m <- 10#
#
# let n be the number of species in Trophic level 2#
n <- 5
mat=matrix(rbinom(100,10,0.1),m,n)
coextDeg(imatrix = mat, nsims = 1000, beta_par1 = 1, beta_par2 = 1)
deg_1_1 <- coextDeg(imatrix = mat, nsims = 1000, beta_par1 = 1, beta_par2 = 1)
# 1 and 1 give uniform-like distribution#
deg_1_1 <- coextDeg(imatrix = mat, nsims = 100, beta_par1 = 1, beta_par2 = 1)#
num_1_1 <- coextNumber(imatrix = mat, nsims = 100, beta_par1 = 1, beta_par2 = 1)#
#
# 0.1 and 1 gives exponential-like distribution;#
deg_0.1_1 <- coextDeg(imatrix = mat, nsims = 100, beta_par1 = 0.1, beta_par2 = 1)#
num_0.1_1 <- coextNumber(imatrix = mat, nsims = 100, beta_par1 = 0.1, beta_par2 = 1)#
#
deg_1_1 <- coextDeg(imatrix = mat, nsims = 100, beta_par1 = 1, beta_par2 = 1)#
num_1_1 <- coextNumber(imatrix = mat, nsims = 100, beta_par1 = 1, beta_par2 = 1)
mat
mat <- matrix(rbinom(100,10,0.1),m,n)
mat
rowSums(mat)
colSums(mat)
c(rowSums(mat), colSums(mat))
min(c(rowSums(mat), colSums(mat)))
if(min(c(rowSums(mat), colSums(mat))) == 0)#
	stop("Hey, you have a species in your network that does not interact with any other species. The following functions will get stuck.")
deg_1_1 <- coextDeg(imatrix = mat, nsims = 100, beta_par1 = 1, beta_par2 = 1)#
num_1_1 <- coextNumber(imatrix = mat, nsims = 100, beta_par1 = 1, beta_par2 = 1)
# 1 and 1 give uniform-like distribution#
deg_1_1 <- coextDeg(imatrix = mat, nsims = 1000, beta_par1 = 1, beta_par2 = 1)#
num_1_1 <- coextNumber(imatrix = mat, nsims = 1000, beta_par1 = 1, beta_par2 = 1)#
#
# 0.1 and 1 gives exponential-like distribution;#
deg_0.1_1 <- coextDeg(imatrix = mat, nsims = 1000, beta_par1 = 0.1, beta_par2 = 1)#
num_0.1_1 <- coextNumber(imatrix = mat, nsims = 1000, beta_par1 = 0.1, beta_par2 = 1)#
#
# 3 and 3 give normal-like distribution#
deg_3_3 <- coextDeg(imatrix = mat, nsims = 1000, beta_par1 = 3, beta_par2 = 3)#
num_3_3 <- coextNumber(imatrix = mat, nsims = 1000, beta_par1 = 3, beta_par2 = 3)
num_3_3
par(mfrow = c(2,3))
par(mfrow = c(2,3))
hist(deg_1_1)
hist(deg_1_1, xlab = "degree", main = "beta(1,1)")
hist(num_1_1, xlab = "degree", main = "beta(1,1) 'uniform'")
pdf(file = "R_from_beta.pdf")#
#
par(mfrow = c(2,3))#
hist(deg_1_1, xlab = "degree", main = "beta(1,1) 'uniform'")#
hist(deg_0.1_1, xlab = "degree", main = "beta(0.1,1) 'exponential'")#
hist(deg_3_3, xlab = "degree", main = "beta(3,3) 'normal'")#
hist(num_1_1, xlab = "degree", main = "beta(1,1) 'uniform'")#
hist(num_0.1_1, xlab = "degree", main = "beta(0.1,1) 'exponential'")#
hist(num_3_3, xlab = "degree", main = "beta(3,3) 'normal'")#
#
dev.off()
pdf(file = "R_from_beta.pdf")#
#
par(mfrow = c(2,3))#
hist(deg_1_1, xlab = "degree", main = "beta(1,1) 'uniform'")#
hist(deg_0.1_1, xlab = "degree", main = "beta(0.1,1) 'exponential'")#
hist(deg_3_3, xlab = "degree", main = "beta(3,3) 'normal'")#
hist(num_1_1, xlab = "number", main = "beta(1,1) 'uniform'")#
hist(num_0.1_1, xlab = "number", main = "beta(0.1,1) 'exponential'")#
hist(num_3_3, xlab = "number", main = "beta(3,3) 'normal'")#
#
dev.off()
par(mfrow = c(2,3))#
hist(deg_1_1, xlab = "degree", main = "beta(1,1) 'uniform'", ylim = c(0,1000))#
hist(deg_0.1_1, xlab = "degree", main = "beta(0.1,1) 'exponential'", ylim = c(0,1000))#
hist(deg_3_3, xlab = "degree", main = "beta(3,3) 'normal'", ylim = c(0,1000))#
hist(num_1_1, xlab = "number", main = "beta(1,1) 'uniform'", ylim = c(0,1000))#
hist(num_0.1_1, xlab = "number", main = "beta(0.1,1) 'exponential'", ylim = c(0,1000))#
hist(num_3_3, xlab = "number", main = "beta(3,3) 'normal'", ylim = c(0,1000))
vec <- round( runif(n = 10, min = -10, max = 10) )
vec
vec_norm <- (vec - min(vec))/(max(vec) - min(vec))
vec_norm
891/40
